{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wtmZm16XrdEi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install portalocker>=2.0.0 torchdata torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ql1TTkpkrjEz",
    "outputId": "61649241-80d6-4ebe-9ff6-7aa4911d884b"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\_meta_registrations.py:26\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[0;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, OrderedDict\n",
    "import numpy as np\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0V3SxHcrllN",
    "outputId": "30a2c141-ad9f-48c6-92b1-4637ec8eae5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sk-Q1W0ustcj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXxCp6Hzruvg"
   },
   "outputs": [],
   "source": [
    "# Define the LSTMModel class, which inherits from nn.Module\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_size=100):\n",
    "        super().__init__()  # Initialize the parent class (nn.Module)\n",
    "\n",
    "        # Define the embedding layer, which converts input tokens into dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, sparse=False)\n",
    "\n",
    "        # Define a bidirectional LSTM layer for processing the embedded sequences\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,  # Size of the input (embedding dimension)\n",
    "            hidden_size=hidden_size,  # Number of units in each LSTM cell\n",
    "            num_layers=1,  # Use a single LSTM layer\n",
    "            batch_first=True,  # Input and output tensors are provided as (batch_size, seq_len, features)\n",
    "            bidirectional=True  # Make the LSTM bidirectional\n",
    "        )\n",
    "\n",
    "        # Define a fully connected layer for classification\n",
    "        # Since the LSTM is bidirectional, the hidden_size is doubled\n",
    "        self.fc = nn.Linear(in_features=hidden_size * 2, out_features=num_class)\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self.init_weights()\n",
    "\n",
    "    # Method to initialize weights of the embedding and fully connected layers\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5  # Define the range for uniform weight initialization\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)  # Initialize embedding weights\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)  # Initialize fully connected layer weights\n",
    "        self.fc.bias.data.zero_()  # Initialize the bias of the fully connected layer to zero\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, text):\n",
    "        # Convert input text (token indices) into dense embeddings\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # Pass the embeddings through the bidirectional LSTM layer\n",
    "        lstm_out, (hn, cn) = self.lstm(embedded)\n",
    "\n",
    "        # Reshape the LSTM output to flatten the sequence dimension for the fully connected layer\n",
    "        # Since the LSTM is bidirectional, the last dimension is doubled\n",
    "        lstm_out = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "\n",
    "        # Pass the flattened output through the fully connected layer to get the final predictions\n",
    "        result = self.fc(lstm_out)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9Kyls4p3xC8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD1gVRAYr1xD"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMDB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m----> 4\u001b[0m train_iter, test_iter \u001b[38;5;241m=\u001b[39m \u001b[43mIMDB\u001b[49m()\n\u001b[0;32m      6\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Turn every single review into a sequence of words, that can then be added to a counter.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IMDB' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "train_iter, test_iter = IMDB()\n",
    "\n",
    "counter = Counter()\n",
    "# Turn every single review into a sequence of words, that can then be added to a counter.\n",
    "for label, text in train_iter:\n",
    "    tokenized_text = tokenize(text)\n",
    "    counter.update(tokenized_text)\n",
    "\n",
    "min_freq = 10\n",
    "max_words = 100000\n",
    "#Adjust the amount of tokens to the maximal defined amount, ranking them according to their occurences in the text\n",
    "ordered_counter = OrderedDict(counter.most_common(max_words))\n",
    "specials = [\"<unk>\"]\n",
    "\n",
    "# Build a vocabulary from the tokenized data, with a maximum of `max_tokens` tokens, and set never seen before words to the unknown index.\n",
    "vocab = vocab(ordered_dict=ordered_counter, min_freq=min_freq, specials=specials)\n",
    "vocab.set_default_index(vocab[specials[0]])\n",
    "\n",
    "# Function to process and collate a batch of data\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        if _label == 1: # This version of the IMDB dataset har 1 for a negative review and 2 for a positive, adjust to make 0 indexed.\n",
    "            label = 0\n",
    "        elif _label == 2:\n",
    "            label = 1\n",
    "        else:\n",
    "            raise ValueError(\"Not a correct review.\") # Check if out schema is correct, catches potential edge cases if wrong IMDB dataset is used.\n",
    "        label_list.append(label)\n",
    "        processed_text = vocab(tokenize(_text))\n",
    "        if len(processed_text) > 200: #This limits every review to have a maximal length of 200 words, also pads short reviews with zeros for consistent length\n",
    "          processed_text = processed_text[:200]\n",
    "        else:\n",
    "          for i in range(200 - len(processed_text)):\n",
    "            processed_text.append(0)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64) #Tensor convertions to make dataset work with pytorch models\n",
    "    text_list = torch.tensor(text_list,dtype=torch.int64)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "# Load the IMDB dataset as iterators\n",
    "train_iter, test_iter = IMDB()\n",
    "\n",
    "# Convert iterators to map-style datasets for DataLoader compatibility\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Create DataLoaders for training and testing with batch processing and custom collate function\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# Determine the number of classes from the training dataset (e.g., positive/negative reviews)\n",
    "num_class = len(set([label for (label, text) in train_iter]))  # Should be 2 for binary classification\n",
    "\n",
    "# Calculate the size of the vocabulary based on the tokens identified earlier\n",
    "vocab_size = len(vocab)  # Example size: 100683 tokens\n",
    "\n",
    "# Define the size of the embedding vectors and initialize the RNN model\n",
    "emsize = 256  # Embedding size for each token\n",
    "model = LSTMModel(vocab_size, emsize, num_class, hidden_size=100).to(device)  # Move the model to the device (e.g., GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TC1lR3-3WKF"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Function to train the model using the given dataloader\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0,0\n",
    "    total_loss = 0\n",
    "    log_interval = 100\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        total_loss += loss.item() * label.shape[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) #Clip gradients to protect us from exploding gradients.\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "\n",
    "# Function to evaluate the model's performance on a validation or test dataset\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_loss += loss.item() * label.shape[0]\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count, total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QN1XE_gXr4tQ",
    "outputId": "1e492021-3faa-4705-9fe9-5ba84e9305bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "| end of epoch   1 | train loss  0.596 | test loss  0.490 \n",
      "| time:      9.55s | train acc.  0.682 | test acc.  0.786 \n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "| end of epoch   2 | train loss  0.385 | test loss  0.449 \n",
      "| time:      9.50s | train acc.  0.841 | test acc.  0.805 \n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "| end of epoch   3 | train loss  0.259 | test loss  0.447 \n",
      "| time:      9.58s | train acc.  0.903 | test acc.  0.817 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.001 --> 0.0009000000000000001\n",
      "----------------------------------------------------------\n",
      "| end of epoch   4 | train loss  0.173 | test loss  0.525 \n",
      "| time:      9.58s | train acc.  0.940 | test acc.  0.810 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.0009000000000000001 --> 0.0008100000000000001\n",
      "----------------------------------------------------------\n",
      "| end of epoch   5 | train loss  0.107 | test loss  0.634 \n",
      "| time:      9.61s | train acc.  0.965 | test acc.  0.817 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.0008100000000000001 --> 0.000729\n",
      "----------------------------------------------------------\n",
      "| end of epoch   6 | train loss  0.067 | test loss  0.869 \n",
      "| time:      9.64s | train acc.  0.979 | test acc.  0.812 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.000729 --> 0.0006561000000000001\n",
      "----------------------------------------------------------\n",
      "| end of epoch   7 | train loss  0.042 | test loss  0.895 \n",
      "| time:      9.65s | train acc.  0.988 | test acc.  0.792 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.0006561000000000001 --> 0.00059049\n",
      "----------------------------------------------------------\n",
      "| end of epoch   8 | train loss  0.023 | test loss  1.120 \n",
      "| time:      9.70s | train acc.  0.993 | test acc.  0.801 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00059049 --> 0.000531441\n",
      "----------------------------------------------------------\n",
      "| end of epoch   9 | train loss  0.014 | test loss  1.384 \n",
      "| time:      9.68s | train acc.  0.996 | test acc.  0.806 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.000531441 --> 0.0004782969\n",
      "----------------------------------------------------------\n",
      "| end of epoch  10 | train loss  0.007 | test loss  1.570 \n",
      "| time:      9.69s | train acc.  0.998 | test acc.  0.806 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.0004782969 --> 0.00043046721\n",
      "----------------------------------------------------------\n",
      "| end of epoch  11 | train loss  0.002 | test loss  1.740 \n",
      "| time:      9.70s | train acc.  0.999 | test acc.  0.808 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00043046721 --> 0.000387420489\n",
      "----------------------------------------------------------\n",
      "| end of epoch  12 | train loss  0.001 | test loss  1.771 \n",
      "| time:      9.71s | train acc.  1.000 | test acc.  0.810 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.000387420489 --> 0.0003486784401\n",
      "----------------------------------------------------------\n",
      "| end of epoch  13 | train loss  0.001 | test loss  1.824 \n",
      "| time:      9.71s | train acc.  1.000 | test acc.  0.811 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.0003486784401 --> 0.00031381059609000004\n",
      "----------------------------------------------------------\n",
      "| end of epoch  14 | train loss  0.001 | test loss  1.954 \n",
      "| time:      9.76s | train acc.  1.000 | test acc.  0.813 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00031381059609000004 --> 0.00028242953648100003\n",
      "----------------------------------------------------------\n",
      "| end of epoch  15 | train loss  0.001 | test loss  1.829 \n",
      "| time:      9.72s | train acc.  1.000 | test acc.  0.808 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00028242953648100003 --> 0.00025418658283290005\n",
      "----------------------------------------------------------\n",
      "| end of epoch  16 | train loss  0.000 | test loss  1.969 \n",
      "| time:      9.72s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00025418658283290005 --> 0.00022876792454961005\n",
      "----------------------------------------------------------\n",
      "| end of epoch  17 | train loss  0.000 | test loss  2.010 \n",
      "| time:      9.71s | train acc.  1.000 | test acc.  0.813 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00022876792454961005 --> 0.00020589113209464906\n",
      "----------------------------------------------------------\n",
      "| end of epoch  18 | train loss  0.000 | test loss  2.051 \n",
      "| time:      9.72s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00020589113209464906 --> 0.00018530201888518417\n",
      "----------------------------------------------------------\n",
      "| end of epoch  19 | train loss  0.000 | test loss  2.077 \n",
      "| time:      9.71s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00018530201888518417 --> 0.00016677181699666576\n",
      "----------------------------------------------------------\n",
      "| end of epoch  20 | train loss  0.000 | test loss  2.107 \n",
      "| time:      9.72s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00016677181699666576 --> 0.0001500946352969992\n",
      "----------------------------------------------------------\n",
      "| end of epoch  21 | train loss  0.000 | test loss  2.132 \n",
      "| time:      9.75s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.0001500946352969992 --> 0.0001350851717672993\n",
      "----------------------------------------------------------\n",
      "| end of epoch  22 | train loss  0.000 | test loss  2.151 \n",
      "| time:      9.69s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.0001350851717672993 --> 0.00012157665459056936\n",
      "----------------------------------------------------------\n",
      "| end of epoch  23 | train loss  0.000 | test loss  2.185 \n",
      "| time:      9.68s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00012157665459056936 --> 0.00010941898913151243\n",
      "----------------------------------------------------------\n",
      "| end of epoch  24 | train loss  0.000 | test loss  2.195 \n",
      "| time:      9.70s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 0.00010941898913151243 --> 9.847709021836118e-05\n",
      "----------------------------------------------------------\n",
      "| end of epoch  25 | train loss  0.000 | test loss  2.219 \n",
      "| time:      9.69s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 9.847709021836118e-05 --> 8.862938119652506e-05\n",
      "----------------------------------------------------------\n",
      "| end of epoch  26 | train loss  0.000 | test loss  2.239 \n",
      "| time:      9.69s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 8.862938119652506e-05 --> 7.976644307687256e-05\n",
      "----------------------------------------------------------\n",
      "| end of epoch  27 | train loss  0.000 | test loss  2.260 \n",
      "| time:      9.68s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 7.976644307687256e-05 --> 7.17897987691853e-05\n",
      "----------------------------------------------------------\n",
      "| end of epoch  28 | train loss  0.000 | test loss  2.278 \n",
      "| time:      9.70s | train acc.  1.000 | test acc.  0.813 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 7.17897987691853e-05 --> 6.461081889226677e-05\n",
      "----------------------------------------------------------\n",
      "| end of epoch  29 | train loss  0.000 | test loss  2.304 \n",
      "| time:      9.71s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n",
      "Adjusting learning rate from 6.461081889226677e-05 --> 5.81497370030401e-05\n",
      "----------------------------------------------------------\n",
      "| end of epoch  30 | train loss  0.000 | test loss  2.323 \n",
      "| time:      9.68s | train acc.  1.000 | test acc.  0.814 \n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30  # Number of epochs to train the model\n",
    "LR = 0.001  # Learning rate for the optimizer\n",
    "BATCH_SIZE = 64  # Number of samples per batch for training\n",
    "\n",
    "# Define the loss function (CrossEntropyLoss) for classification tasks\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent) with the specified learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Define a learning rate scheduler that decays the learning rate by a factor of 0.1 every epoch\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Variable to track the best validation accuracy\n",
    "total_accu = None\n",
    "\n",
    "# Load the IMDB dataset and convert iterators to map-style datasets\n",
    "train_iter, test_iter = IMDB()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# Training loop over the specified number of epochs\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()  # Record the start time of the epoch\n",
    "\n",
    "    # Train the model for one epoch and get the training accuracy and loss\n",
    "    accu_train, loss_train = train(train_dataloader)\n",
    "\n",
    "    # Evaluate the model on the validation set and get the validation accuracy and loss\n",
    "    accu_test, loss_test = evaluate(test_dataloader)\n",
    "\n",
    "    # Adjust the learning rate if the validation accuracy does not improve\n",
    "    if total_accu is not None and total_accu > accu_test:\n",
    "        old_LR = scheduler.optimizer.param_groups[0]['lr']\n",
    "        scheduler.step()  # Decay the learning rate\n",
    "        new_LR = scheduler.optimizer.param_groups[0]['lr']\n",
    "        print(f\"Adjusting learning rate from {old_LR} --> {new_LR}\")\n",
    "    else:\n",
    "        total_accu = accu_test  # Update the best validation accuracy\n",
    "\n",
    "    # Print the results for the current epoch\n",
    "    print(\"-\" * 58)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | train loss {:6.3f} | \"\n",
    "        \"test loss {:6.3f} \".format(\n",
    "            epoch, loss_train, loss_test\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"| time: {:9.2f}s | train acc. {:6.3f} | \"\n",
    "        \"test acc. {:6.3f} \".format(\n",
    "            time.time() - epoch_start_time, accu_train, accu_test\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 58)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
